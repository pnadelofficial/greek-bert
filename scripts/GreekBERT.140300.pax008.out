Starting
Wed Dec 17 04:34:55 PM EST 2025
Module loading
Activating env
Starting BERT Training
Training on 8 GPUs with mixed precision
Local rank: 0, Global rank: 0
TensorBoard logs will be saved to: ./runs
Run: tensorboard --logdir=./runs
Loading dataset...
Dataset info:
  Type: <class 'datasets.arrow_dataset.Dataset'>
  Length: 1374466, type: <class 'int'>
  Features: {'input_ids': List(Value('int32')), 'attention_mask': List(Value('int8')), 'labels': List(Value('int64'))}

Sample data:
  input_ids: type=<class 'torch.Tensor'>, dtype=torch.int64
  attention_mask: type=<class 'torch.Tensor'>, dtype=torch.int64
  labels: type=<class 'torch.Tensor'>, dtype=torch.int64
Initializing model...
Tokenizing dataset...
DEBUG: num_epochs = 30, type = <class 'int'>
DEBUG: len(train_dataloader) = 5370, type = <class 'int'>
DEBUG: gradient_accumulation_steps = 1, type = <class 'int'>
DEBUG: num_training_steps = 161100, type = <class 'int'>
DEBUG: lr = 5e-5, type = <class 'str'>
DEBUG: max_lr = 1e-4, type = <class 'str'>
DEBUG: pct_start = 0.05, type = <class 'float'>
DEBUG: weight_decay = 0.01, type = <class 'float'>

Starting training for 30 epochs
Total training steps: 161100
Steps per epoch: 5370
Effective batch size: 256
Mixed precision: True
Gradient accumulation steps: 1

Epoch 1/30
Epoch 1 - Average Loss: 3.1247
Running validation...
Validation Loss: 2.5791

Epoch 2/30
Epoch 2 - Average Loss: 2.5255
Running validation...
Validation Loss: 2.2865

Epoch 3/30
Epoch 3 - Average Loss: 2.3251
Running validation...
Validation Loss: 2.1550

Epoch 4/30
Epoch 4 - Average Loss: 2.2278
Running validation...
Validation Loss: 2.0725

Epoch 5/30
Epoch 5 - Average Loss: 2.1595
Checkpoint saved to: ./checkpoints/checkpoint_epoch_5.pt
Running validation...
Validation Loss: 2.0226

Epoch 6/30
Epoch 6 - Average Loss: 2.1096
Running validation...
Validation Loss: 1.9788

Epoch 7/30
Epoch 7 - Average Loss: 2.0737
Running validation...
Validation Loss: 1.9402

Epoch 8/30
Epoch 8 - Average Loss: 2.0400
Running validation...
Validation Loss: 1.9134

Epoch 9/30
Epoch 9 - Average Loss: 2.0110
Running validation...
Validation Loss: 1.8896

Epoch 10/30
Epoch 10 - Average Loss: 1.9901
Checkpoint saved to: ./checkpoints/checkpoint_epoch_10.pt
Running validation...
Validation Loss: 1.8672

Epoch 11/30
Epoch 11 - Average Loss: 1.9706
Running validation...
Validation Loss: 1.8469

Epoch 12/30
Epoch 12 - Average Loss: 1.9495
Running validation...
Validation Loss: 1.8293

Epoch 13/30
Epoch 13 - Average Loss: 1.9324
Running validation...
Validation Loss: 1.8133

Epoch 14/30
Epoch 14 - Average Loss: 1.9162
Running validation...
Validation Loss: 1.7976

Epoch 15/30
Epoch 15 - Average Loss: 1.8998
Checkpoint saved to: ./checkpoints/checkpoint_epoch_15.pt
Running validation...
Validation Loss: 1.7817

Epoch 16/30
Epoch 16 - Average Loss: 1.8921
Running validation...
Validation Loss: 1.7707

Epoch 17/30
Epoch 17 - Average Loss: 1.8757
Running validation...
Validation Loss: 1.7580

Epoch 18/30
Epoch 18 - Average Loss: 1.8636
Running validation...
Validation Loss: 1.7488

Epoch 19/30
Epoch 19 - Average Loss: 1.8543
Running validation...
Validation Loss: 1.7392

Epoch 20/30
Epoch 20 - Average Loss: 1.8428
Checkpoint saved to: ./checkpoints/checkpoint_epoch_20.pt
Running validation...
Validation Loss: 1.7297

Epoch 21/30
Epoch 21 - Average Loss: 1.8363
Running validation...
Validation Loss: 1.7233

Epoch 22/30
Epoch 22 - Average Loss: 1.8299
Running validation...
Validation Loss: 1.7166

Epoch 23/30
Epoch 23 - Average Loss: 1.8215
Running validation...
Validation Loss: 1.7086

Epoch 24/30
Epoch 24 - Average Loss: 1.8155
Running validation...
Validation Loss: 1.7058

Epoch 25/30
Epoch 25 - Average Loss: 1.8093
Checkpoint saved to: ./checkpoints/checkpoint_epoch_25.pt
Running validation...
Validation Loss: 1.7007

Epoch 26/30
Epoch 26 - Average Loss: 1.8084
Running validation...
Validation Loss: 1.6980

Epoch 27/30
Epoch 27 - Average Loss: 1.8036
Running validation...
Validation Loss: 1.6951

Epoch 28/30
Epoch 28 - Average Loss: 1.8015
Running validation...
Validation Loss: 1.6926

Epoch 29/30
Epoch 29 - Average Loss: 1.8022
Running validation...
Validation Loss: 1.6952

Epoch 30/30
Epoch 30 - Average Loss: 1.8022
Checkpoint saved to: ./checkpoints/checkpoint_epoch_30.pt
Running validation...
Validation Loss: 1.6966

Training complete! Final model saved to: ./checkpoints/final_model.pt
Training finished
Converting model to HuggingFace format
Post-training with spaCy
Activating env
Starting post-training with spaCy
[38;5;4mâ„¹ Saving to output directory: output[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'tagger', 'morphologizer',
'trainable_lemmatizer', 'parser'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS TAGGER  LOSS MORPH...  LOSS TRAIN...  LOSS PARSER  TAG_ACC  POS_ACC  MORPH_ACC  LEMMA_ACC  DEP_UAS  DEP_LAS  SENTS_F  SCORE 
---  ------  -------------  -----------  -------------  -------------  -----------  -------  -------  ---------  ---------  -------  -------  -------  ------
  0       0        1916.84      1483.22        1483.98        1484.80      2716.96     0.00    18.20       0.99      20.31     6.23     6.23     0.05    0.09
  1     200      308602.78    355563.55      356271.17      356794.40    490173.36     8.14    12.11      32.12      20.28     9.67     5.18     0.03    0.14
  2     400      268879.18    342978.78      351258.85      353711.99    333080.35     8.14    12.11      32.12      20.28    25.23    15.47     0.95    0.18
  3     600      277594.62    315465.07      335441.79      334626.81    314086.56    34.10    36.31      33.15      20.29    37.51    25.97     7.56    0.30
  4     800      306134.42    263995.51      306191.55      312092.63    301740.80    39.65    58.24      35.67      26.23    45.36    33.85    23.18    0.38
  5    1000      336271.96    215507.01      276234.28      287935.34    288738.44    48.94    67.01      40.10      32.85    50.33    39.74    31.66    0.45
  6    1200      367290.67    181257.58      255894.12      270524.76    274240.19    55.93    75.11      51.22      40.35    54.62    44.54    37.79    0.52
  7    1400      404196.54    152776.37      228947.59      247022.29    258872.97    67.18    80.66      63.06      46.69    58.47    49.00    53.67    0.60
  8    1600      433008.08    126705.83      198599.35      228574.92    244746.42    74.80    87.01      70.79      51.86    60.17    51.40    50.41    0.65
  9    1800      466511.68    100188.53      169571.42      209463.59    234576.30    78.38    88.73      74.76      55.52    62.16    53.92    64.84    0.68
 10    2000      477767.92     82736.30      146023.01      195130.08    221944.03    81.52    90.65      77.55      57.43    64.28    56.24    64.09    0.71
 11    2200      489670.00     67771.15      127828.91      180279.11    211765.31    83.88    92.37      80.36      59.50    65.17    57.59    65.18    0.73
 12    2400      488879.74     57035.90      110832.11      167263.92    198094.95    85.16    93.25      82.18      60.87    64.63    57.51    59.91    0.74
 13    2600      496350.30     49662.43       98840.92      157649.40    191936.89    86.27    93.61      83.75      62.77    66.53    59.17    68.54    0.75
 14    2800      495447.55     42081.40       87338.04      149341.25    181203.14    86.81    93.85      85.20      64.09    67.25    60.00    65.23    0.76
 15    3000      501190.62     36954.68       78088.05      140904.58    174758.56    87.46    94.09      86.33      65.36    66.47    59.82    68.51    0.77
 16    3200      464153.04     31628.27       69191.77      133013.27    158954.41    87.83    94.07      87.12      66.03    68.66    61.63    67.39    0.77
 17    3400      474689.77     28902.77       62784.40      127114.27    154796.83    88.53    94.45      87.89      67.15    69.29    62.29    73.12    0.78
 18    3600      453225.13     24726.98       57201.07      122442.29    147377.05    88.83    94.55      88.37      68.03    69.59    62.88    69.08    0.79
 20    3800      414246.68     21371.21       50184.93      113938.38    134105.24    88.78    94.25      88.88      69.03    69.42    62.78    67.06    0.79
 21    4000      400090.85     19126.64       45884.76      109132.84    129142.74    89.38    94.64      89.19      69.33    69.62    63.08    73.68    0.79
 22    4200      382149.17     16831.86       41803.46      104760.96    121641.71    89.72    94.79      89.52      70.38    69.33    62.80    70.79    0.80
 23    4400      374122.10     15111.54       38287.40       99830.86    115232.62    89.50    94.64      89.51      70.70    69.70    62.84    73.59    0.80
 24    4600      346694.70     13368.27       35402.82       94915.02    108363.42    89.84    94.76      90.03      71.08    70.37    63.51    70.44    0.80
 25    4800      339005.60     11977.89       32256.38       89758.50    103972.86    89.78    94.65      89.96      72.41    69.43    62.74    72.32    0.80
 26    5000      311766.91     10693.30       29689.66       85242.72     97643.43    89.97    94.92      90.27      72.79    70.50    63.92    72.66    0.81
 27    5200      291751.05      9690.25       27108.89       80840.97     92021.95    90.07    94.91      90.22      73.47    69.94    63.22    74.41    0.81
 28    5400      290989.08      8935.33       25805.43       76667.17     89048.00    89.83    94.71      90.19      73.93    69.85    63.28    71.98    0.81
 29    5600      260576.66      7895.23       23511.89       72144.36     81781.68    89.92    94.55      90.33      74.28    69.88    63.28    69.79    0.81
 30    5800      276965.70      7513.74       22339.59       70132.09     80628.69    90.02    94.79      90.49      74.56    70.06    63.38    74.19    0.81
 31    6000      238662.99      6362.77       20425.69       64866.38     75078.36    90.08    94.74      90.62      74.92    70.10    63.40    73.94    0.81
 32    6200      244634.05      6146.26       18866.59       62238.32     72054.14    90.20    94.86      90.55      74.99    69.90    63.20    71.54    0.81
 33    6400      241325.08      5419.40       18122.29       59805.79     70291.57    90.20    94.82      90.67      75.59    69.48    62.91    73.70    0.81
 34    6600      217407.72      4860.90       16282.90       55816.75     65568.21    90.12    94.86      90.69      75.91    70.16    63.35    71.45    0.81
 35    6800      209626.24      4363.13       15674.06       52287.75     62882.90    90.05    94.66      90.68      76.72    69.89    63.15    73.48    0.82
 36    7000      207487.18      4232.03       14343.56       50695.74     61059.39    90.12    94.79      90.66      76.78    69.17    62.59    73.11    0.81
 37    7200      204130.44      3678.11       13620.99       47122.14     58050.51    89.93    94.47      90.65      76.91    69.45    62.66    72.47    0.81
 38    7400      180971.28      3457.81       12713.76       45027.00     55166.54    90.17    94.79      90.64      77.37    69.70    62.98    68.52    0.82
 39    7600      170897.46      3262.94       11919.67       43090.67     53117.02    89.82    94.51      90.52      77.44    69.47    62.66    70.63    0.82
 40    7800      172829.80      2911.82       11126.34       40156.45     51096.64    90.05    94.58      90.68      77.70    69.85    62.88    71.50    0.82
 42    8000      164718.94      2830.08       10634.75       38164.19     48566.17    90.08    94.69      90.65      78.12    69.82    62.89    72.34    0.82
 43    8200      148604.12      2535.69        9780.38       36124.34     46384.14    89.59    94.36      90.53      78.00    69.35    62.48    73.30    0.82
 44    8400      142441.29      2406.78        9340.09       34092.07     43791.83    90.25    94.73      90.73      78.31    69.61    62.55    72.89    0.82
 45    8600      136614.99      2194.96        8514.38       31654.55     42322.78    89.92    94.60      90.58      78.52    69.74    62.88    72.24    0.82
 46    8800      127508.41      2004.59        8107.82       30178.67     39991.70    89.97    94.40      90.48      78.66    69.12    62.12    72.86    0.82
 47    9000      126524.49      1785.80        7585.16       28194.32     39090.81    89.96    94.69      90.40      78.96    69.28    62.26    72.07    0.82
 48    9200      126877.40      1562.91        6920.16       26363.66     38097.17    89.91    94.58      90.67      78.72    69.60    62.55    73.90    0.82
 49    9400      121311.43      1456.34        6567.60       24531.71     35949.83    89.80    94.51      90.50      79.10    69.36    62.31    71.61    0.82
 50    9600      108674.91      1216.93        5801.47       22649.42     33892.42    89.96    94.66      90.51      79.25    68.94    61.97    72.54    0.82
 51    9800      104311.18      1155.91        5301.89       21098.81     33366.51    89.79    94.50      90.38      79.24    68.90    61.75    72.30    0.82
 52   10000       98255.43      1027.34        4804.05       19435.76     32292.73    90.09    94.58      90.66      79.36    69.11    62.25    73.02    0.82
 53   10200       97537.63       893.33        4582.43       18042.87     30597.88    89.63    94.32      90.45      79.57    68.60    61.46    70.06    0.82
 54   10400       89766.22       822.45        4015.32       16269.86     29275.28    90.05    94.60      90.61      79.47    68.90    61.88    72.86    0.82
 55   10600       86118.36       743.80        3759.27       15355.44     28888.25    89.93    94.56      90.51      79.74    68.62    61.56    71.43    0.82
 56   10800       74862.67       645.61        3456.55       14132.17     27600.42    89.88    94.48      90.59      79.98    69.27    62.00    72.18    0.82
 57   11000       74414.37       585.64        3075.42       12829.02     27476.76    89.67    94.35      90.46      79.56    68.72    61.63    72.27    0.82
 58   11200       68399.02       540.15        2797.47       11978.28     26938.05    89.73    94.43      90.54      80.30    68.86    61.80    72.67    0.82
 60   11400       69852.62       453.54        2462.37       10743.32     26802.08    89.87    94.46      90.59      79.84    69.21    62.09    72.19    0.82
 61   11600       66097.29       432.01        2253.55        9940.23     25569.62    89.98    94.41      90.53      80.32    69.15    62.32    73.66    0.82
 62   11800       69307.72       370.98        2107.52        9151.83     25798.76    89.86    94.49      90.47      80.37    69.19    62.01    71.97    0.82
 63   12000       63431.43       349.76        1893.03        8506.62     25117.36    89.89    94.37      90.57      80.12    68.58    61.68    72.03    0.82
 64   12200       67892.24       327.55        1824.77        7595.20     25548.00    89.83    94.29      90.57      80.03    69.14    62.06    73.84    0.82
 65   12400       65508.83       270.23        1644.53        6974.15     25642.30    89.81    94.41      90.40      80.48    68.70    61.56    70.65    0.82
 66   12600       67318.41       254.35        1439.15        6424.46     24870.57    89.83    94.36      90.49      80.98    68.04    60.99    72.97    0.82
 67   12800       80141.58       228.98        1354.09        5856.24     25640.67    89.76    94.36      90.46      80.33    68.73    61.69    71.76    0.82
 68   13000       75528.38       204.24        1273.67        5496.76     25140.97    89.89    94.51      90.42      80.80    68.83    61.66    71.69    0.82
 69   13200       58146.93       176.80        1142.99        4854.13     24076.64    89.77    94.42      90.38      80.81    69.01    61.92    73.62    0.82
[38;5;2mâœ” Saved pipeline to output directory[0m
output/model-last
Post-training finished
Evaluating spaCy model
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
================================== Results ==================================[0m

TOK      99.99
TAG      88.51
POS      93.41
MORPH    89.64
LEMMA    79.50
UAS      68.26
LAS      61.37
SENT P   71.17
SENT R   72.59
SENT F   71.87
SPEED    5455 

[1m
============================== MORPH (per feat) ==============================[0m

                P       R       F
Mood        96.35   95.82   96.09
Number      98.28   97.60   97.94
Person      96.30   94.19   95.23
Tense       94.96   94.16   94.56
VerbForm    97.51   96.72   97.11
Voice       94.67   93.86   94.26
Case        96.06   95.40   95.73
Gender      91.56   91.04   91.30
Aspect      92.64   92.72   92.68
PronType    96.94   96.45   96.70
Reflex      93.62   95.65   94.62
Degree      82.58   80.65   81.60
Definite    98.01   98.89   98.45
Polarity    99.52   94.57   96.98
Poss       100.00   38.46   55.56

[1m
=============================== LAS (per type) ===============================[0m

                  P       R       F
root          75.70   77.18   76.43
advmod        57.93   53.94   55.86
nsubj         59.16   59.38   59.27
det           82.29   81.57   81.93
nmod          48.52   51.72   50.07
obj           58.62   60.93   59.75
advcl         53.49   50.64   52.02
cc            61.35   65.30   63.26
conj          43.10   41.18   42.12
amod          48.43   41.73   44.83
cop           60.76   55.31   57.91
obl           54.80   50.70   52.67
case          86.55   86.43   86.49
xcomp         32.04   35.92   33.87
csubj         11.69   18.75   14.40
mark          70.02   70.86   70.44
ccomp         37.86   35.02   36.39
dep            0.00    0.00    0.00
iobj          37.93   33.33   35.48
acl           24.81   19.22   21.66
discourse     71.24   69.80   70.51
vocative      37.96   35.34   36.61
nummod        55.32   71.23   62.28
appos         22.98   21.26   22.09
parataxis      3.12    3.12    3.12
csubj:pass     0.00    0.00    0.00
orphan        10.20    7.69    8.77
nsubj:outer    0.00    0.00    0.00
obl:arg       61.63   62.88   62.25
nsubj:pass    54.02   49.47   51.65
dislocated     4.35    3.70    4.00
advcl:cmp     25.00   19.05   21.62
obl:agent     31.82   31.82   31.82
fixed         20.00    9.09   12.50
flat:name     66.67   57.14   61.54
aux            0.00    0.00    0.00
aux:pass       0.00    0.00    0.00

Evaluation finished
