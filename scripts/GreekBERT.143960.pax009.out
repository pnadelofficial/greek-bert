Starting
Thu Jan  8 12:43:44 PM EST 2026
Module loading
Activating env
Starting BERT Training
Training on 8 GPUs with mixed precision
Local rank: 0, Global rank: 0
TensorBoard logs will be saved to: ./runs
Run: tensorboard --logdir=./runs
Loading dataset...
Dataset info:
  Type: <class 'datasets.arrow_dataset.Dataset'>
  Length: 1360380, type: <class 'int'>
  Features: {'input_ids': List(Value('int32')), 'attention_mask': List(Value('int8')), 'labels': List(Value('int64'))}

Sample data:
  input_ids: type=<class 'torch.Tensor'>, dtype=torch.int64
  attention_mask: type=<class 'torch.Tensor'>, dtype=torch.int64
  labels: type=<class 'torch.Tensor'>, dtype=torch.int64
Initializing model...
Tokenizing dataset...
DEBUG: num_epochs = 10, type = <class 'int'>
DEBUG: len(train_dataloader) = 5314, type = <class 'int'>
DEBUG: gradient_accumulation_steps = 1, type = <class 'int'>
DEBUG: num_training_steps = 53140, type = <class 'int'>
DEBUG: lr = 5e-5, type = <class 'str'>
DEBUG: max_lr = 1e-4, type = <class 'str'>
DEBUG: pct_start = 0.05, type = <class 'float'>
DEBUG: weight_decay = 0.01, type = <class 'float'>

Starting training for 10 epochs
Total training steps: 53140
Steps per epoch: 5314
Effective batch size: 256
Mixed precision: True
Gradient accumulation steps: 1

Epoch 1/10
Epoch 1 - Average Loss: 2.6941
Running validation...
Validation Loss: 2.2780

Epoch 2/10
Epoch 2 - Average Loss: 2.3233
Running validation...
Validation Loss: 2.1579

Epoch 3/10
Epoch 3 - Average Loss: 2.2302
Running validation...
Validation Loss: 2.0836

Epoch 4/10
Epoch 4 - Average Loss: 2.1691
Running validation...
Validation Loss: 2.0315

Epoch 5/10
Epoch 5 - Average Loss: 2.1238
Checkpoint saved to: ./checkpoints/checkpoint_epoch_5.pt
Running validation...
Validation Loss: 1.9926

Epoch 6/10
Epoch 6 - Average Loss: 2.0907
Running validation...
Validation Loss: 1.9615

Epoch 7/10
Epoch 7 - Average Loss: 2.0649
Running validation...
Validation Loss: 1.9428

Epoch 8/10
Epoch 8 - Average Loss: 2.0448
Running validation...
Validation Loss: 1.9289

Epoch 9/10
Epoch 9 - Average Loss: 2.0333
Running validation...
Validation Loss: 1.9217

Epoch 10/10
Epoch 10 - Average Loss: 2.0279
Checkpoint saved to: ./checkpoints/checkpoint_epoch_10.pt
Running validation...
Validation Loss: 1.9214

Training complete! Final model saved to: ./checkpoints/final_model.pt
Training finished
Converting model to HuggingFace format
Post-training with spaCy
Activating env
Starting post-training with spaCy
[38;5;4mâ„¹ Saving to output directory: output[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'tagger', 'morphologizer',
'trainable_lemmatizer', 'parser'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS TAGGER  LOSS MORPH...  LOSS TRAIN...  LOSS PARSER  TAG_ACC  POS_ACC  MORPH_ACC  LEMMA_ACC  DEP_UAS  DEP_LAS  SENTS_F  SCORE 
---  ------  -------------  -----------  -------------  -------------  -----------  -------  -------  ---------  ---------  -------  -------  -------  ------
  0       0        1651.33      1483.22        1483.98        1484.80      2510.64     0.00    18.20       0.99      20.31    13.00     2.86     0.06    0.09
  1     200      353485.41    353724.86      355531.65      356552.21    377990.58    15.04    12.11      32.12      20.31    47.02    31.77     4.13    0.24
  2     400      242641.39    281477.73      319214.58      333049.89    264227.89    46.08    87.83      45.82      30.15    67.54    57.99    57.48    0.51
  3     600      219099.53    135755.39      209134.33      258616.93    213177.20    80.71    94.78      74.99      54.59    72.41    64.92    71.93    0.72
  4     800      209826.67     61797.56      117442.07      194781.10    184953.14    89.47    96.56      86.89      61.41    75.78    68.82    78.19    0.79
  5    1000      192974.78     38134.83       73996.23      162636.45    159943.24    91.88    97.07      90.46      64.70    77.93    71.47    79.89    0.81
  6    1200      189328.10     28780.49       56061.39      146261.82    147764.87    93.08    97.30      92.10      67.42    79.09    73.23    81.85    0.83
  7    1400      179604.61     22922.75       45476.79      131690.06    134804.37    94.02    97.44      92.96      69.85    79.93    74.25    83.33    0.84
  8    1600      171809.68     19671.12       38564.07      122816.64    125085.06    94.43    97.57      93.55      71.58    80.81    75.42    83.36    0.85
  9    1800      162848.97     16150.81       33571.00      111009.42    116799.39    94.57    97.65      93.96      72.94    81.32    76.05    85.11    0.85
 10    2000      154119.68     14503.67       29478.14      102858.78    108686.48    94.90    97.71      94.22      74.46    82.00    76.89    84.77    0.86
 11    2200      148568.73     12586.98       26891.89       93981.34    102545.62    94.96    97.75      94.46      75.70    82.11    77.00    85.89    0.87
 12    2400      139436.74     11136.90       23902.40       86035.60     94794.71    94.98    97.75      94.63      76.88    82.21    77.28    84.80    0.87
 13    2600      133070.24     10069.95       21765.65       80035.13     91087.31    95.23    97.83      94.71      77.70    82.67    77.81    85.17    0.87
 14    2800      126804.33      9089.82       20332.81       74088.39     85924.70    95.02    97.62      94.62      78.66    82.72    77.89    85.83    0.87
 15    3000      122396.45      8129.81       18393.60       68053.10     81750.94    95.09    97.72      94.67      79.29    83.03    78.22    86.07    0.88
 16    3200      111803.09      7110.16       16659.25       62945.18     74937.82    95.14    97.73      94.77      80.04    83.20    78.50    85.14    0.88
 17    3400      110235.68      6745.77       15679.54       58229.14     73060.53    95.27    97.85      94.84      80.65    83.43    78.90    85.81    0.88
 18    3600      106026.81      5970.57       14544.51       53921.63     70777.79    95.26    97.83      94.79      81.25    83.47    78.84    85.81    0.88
 20    3800       98648.74      5489.41       13285.36       48817.57     65231.88    95.27    97.77      94.90      81.84    83.21    78.62    86.14    0.89
 21    4000       94639.31      4901.29       12241.66       44905.46     63181.97    95.32    97.83      94.98      82.27    83.48    78.93    86.25    0.89
 22    4200       91456.76      4511.15       11441.44       41442.73     61269.83    95.22    97.79      94.94      82.67    83.57    78.96    85.52    0.89
 23    4400       87305.06      4138.19       10717.76       38030.60     58102.72    95.32    97.85      94.95      82.94    83.19    78.65    85.83    0.89
 24    4600       84669.89      3753.38        9985.17       34851.54     56396.89    95.38    97.89      94.93      83.19    83.72    79.22    86.87    0.89
 25    4800       77997.81      3390.63        9111.40       31370.49     53518.90    95.24    97.83      95.00      83.51    83.79    79.34    85.33    0.89
 26    5000       75958.65      3052.38        8378.97       29036.91     52001.81    95.39    97.87      95.03      83.75    83.61    79.03    86.35    0.89
 27    5200       72749.69      2914.30        7864.63       26268.71     49933.60    95.33    97.87      94.97      83.97    83.86    79.40    86.59    0.89
 28    5400       71935.39      2654.20        7419.42       23685.83     48993.18    95.41    97.84      95.15      84.10    83.64    79.34    86.33    0.89
 29    5600       64360.37      2445.61        6876.39       21654.52     45429.33    95.37    97.87      95.05      84.41    83.86    79.31    86.57    0.89
 30    5800       63664.27      2258.32        6365.82       20011.04     44666.78    95.22    97.75      95.05      84.46    83.91    79.36    86.18    0.89
 31    6000       62207.25      2007.37        5828.00       17781.84     43653.89    95.38    97.89      95.07      84.81    83.89    79.45    85.77    0.90
 32    6200       59361.21      1963.41        5485.69       16491.69     41754.09    95.35    97.81      95.04      84.81    83.84    79.43    86.19    0.90
 33    6400       57805.00      1728.05        5087.89       15049.67     41061.91    95.33    97.84      95.05      84.86    83.88    79.50    86.49    0.90
 34    6600       53458.16      1609.90        4643.62       13617.44     39427.51    95.29    97.82      95.08      84.94    84.18    79.82    86.13    0.90
 35    6800       51979.66      1419.37        4417.65       12177.01     38088.54    95.38    97.85      95.11      85.03    84.02    79.55    85.99    0.90
 36    7000       50777.98      1423.73        4002.78       11313.86     37863.85    95.40    97.89      95.14      85.12    83.99    79.58    86.32    0.90
 37    7200       48270.55      1178.72        3723.69       10003.60     35869.78    95.37    97.87      95.14      85.09    84.19    79.81    86.25    0.90
 38    7400       47698.28      1206.21        3577.64        9321.55     35777.20    95.40    97.85      95.27      85.19    84.32    79.85    86.43    0.90
 39    7600       44077.41      1110.43        3303.70        8637.54     34610.82    95.41    97.86      95.24      85.25    84.06    79.70    85.95    0.90
 40    7800       43973.53      1021.34        3022.41        7658.22     34112.90    95.46    97.90      95.17      85.32    84.25    79.84    86.22    0.90
 42    8000       41151.16       924.60        2758.90        6915.50     32522.30    95.37    97.83      95.16      85.41    83.99    79.54    85.71    0.90
 43    8200       40093.83       862.67        2532.14        6378.25     32530.46    95.46    97.87      95.26      85.49    83.93    79.64    86.08    0.90
 44    8400       38945.11       826.60        2454.19        5788.33     31495.01    95.46    97.91      95.26      85.55    84.21    79.84    85.98    0.90
 45    8600       36787.80       698.51        2128.58        5338.72     30947.74    95.35    97.82      95.16      85.55    83.94    79.45    86.95    0.90
 46    8800       35752.42       705.25        2056.63        4882.92     29949.22    95.46    97.83      95.25      85.64    83.91    79.54    86.87    0.90
 47    9000       34599.38       630.80        1977.32        4471.46     29987.60    95.49    97.90      95.16      85.53    84.13    79.73    87.06    0.90
 48    9200       32443.28       582.95        1744.73        4180.30     29376.67    95.41    97.84      95.17      85.60    84.37    79.93    86.79    0.90
 49    9400       32742.09       616.10        1731.32        3764.75     28644.64    95.34    97.75      95.20      85.62    84.16    79.65    86.74    0.90
 50    9600       31733.50       492.79        1522.70        3467.90     28301.36    95.34    97.81      95.19      85.74    84.10    79.66    86.92    0.90
 51    9800       31179.87       522.98        1445.34        3187.40     28214.20    95.46    97.95      95.26      85.76    84.18    79.74    86.37    0.90
 52   10000       29794.92       466.61        1340.70        2909.93     28072.11    95.32    97.83      95.15      85.72    84.05    79.68    86.25    0.90
 53   10200       28868.63       445.66        1337.26        2750.76     27079.48    95.32    97.87      95.19      85.76    84.20    79.80    86.12    0.90
 54   10400       27512.90       397.05        1185.54        2452.03     26691.40    95.45    97.91      95.18      85.69    84.14    79.77    85.67    0.90
 55   10600       26493.41       402.96        1130.91        2370.02     26511.53    95.46    97.93      95.25      85.74    84.28    79.94    87.27    0.90
 56   10800       26377.90       408.05        1087.96        2204.22     26190.70    95.46    97.87      95.24      85.84    84.22    79.93    86.52    0.90
 57   11000       25513.81       336.64         984.95        2034.03     26325.10    95.36    97.82      95.19      85.80    84.39    80.07    87.19    0.90
 58   11200       25766.35       350.35         929.56        1961.16     26265.34    95.40    97.88      95.21      85.79    84.11    79.71    86.30    0.90
 60   11400       24963.05       316.30         901.56        1785.18     26392.93    95.40    97.85      95.23      85.83    84.39    80.05    86.49    0.90
 61   11600       23300.03       306.79         817.67        1718.58     25312.85    95.41    97.89      95.19      85.93    84.53    80.20    86.92    0.90
 62   11800       24119.06       298.32         812.95        1585.26     25461.12    95.46    97.90      95.15      85.84    84.31    79.98    86.51    0.90
 63   12000       22330.11       263.57         736.45        1502.15     24942.39    95.42    97.88      95.22      85.88    84.43    80.07    86.46    0.90
 64   12200       21910.38       239.17         692.57        1361.05     24951.64    95.40    97.86      95.17      85.86    84.44    80.24    86.71    0.90
 65   12400       22393.99       253.80         670.22        1329.73     25552.68    95.33    97.83      95.15      85.88    84.29    80.05    86.88    0.90
 66   12600       20893.64       251.84         606.94        1225.61     24414.27    95.46    97.92      95.18      85.91    84.34    80.11    86.80    0.90
 67   12800       19398.08       231.32         593.49        1200.34     24564.76    95.41    97.92      95.23      85.96    84.37    80.02    86.65    0.90
 68   13000       20518.23       202.57         582.53        1127.31     24328.16    95.39    97.89      95.17      85.91    84.27    80.02    86.76    0.90
 69   13200       19813.09       212.49         547.84        1054.69     24369.28    95.43    97.94      95.26      85.88    84.39    80.16    87.06    0.90
[38;5;2mâœ” Saved pipeline to output directory[0m
output/model-last
Post-training finished
Evaluating spaCy model
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
================================== Results ==================================[0m

TOK      99.99
TAG      94.16
POS      97.10
MORPH    94.30
LEMMA    84.34
UAS      83.77
LAS      79.58
SENT P   86.31
SENT R   87.08
SENT F   86.69
SPEED    11786

[1m
============================== MORPH (per feat) ==============================[0m

                P        R        F
Mood        98.46    98.20    98.33
Number      99.02    98.72    98.87
Person      98.47    97.05    97.76
Tense       97.66    97.46    97.56
VerbForm    99.18    99.06    99.12
Voice       98.03    97.84    97.93
Case        98.06    97.77    97.91
Gender      94.80    94.69    94.75
Aspect      97.48    97.57    97.53
Degree      94.80    95.69    95.24
PronType    98.99    98.43    98.71
Definite    99.61    99.39    99.50
Reflex     100.00   100.00   100.00
Polarity    99.54    98.64    99.09
Poss        92.31    92.31    92.31

[1m
=============================== LAS (per type) ===============================[0m

                  P       R       F
root          89.81   90.61   90.21
advmod        75.54   73.76   74.64
nsubj         81.04   80.71   80.87
det           92.14   91.35   91.74
nmod          71.87   70.85   71.36
obj           81.02   81.46   81.24
advcl         71.67   74.11   72.87
cc            82.90   84.40   83.64
conj          71.86   71.02   71.44
amod          70.65   59.35   64.51
cop           75.10   78.16   76.60
obl           72.58   72.88   72.73
case          94.79   94.79   94.79
xcomp         59.92   60.39   60.16
csubj         59.18   60.42   59.79
mark          90.21   88.97   89.58
ccomp         67.44   63.53   65.42
iobj          56.04   51.52   53.68
dep            0.00    0.00    0.00
acl           51.35   45.65   48.33
discourse     85.53   85.53   85.53
vocative      69.23   62.07   65.45
nummod        68.89   84.93   76.07
appos         44.94   40.80   42.77
parataxis     20.00    9.38   12.77
dislocated    35.71   18.52   24.39
obl:arg       81.41   81.82   81.61
orphan        31.37   24.62   27.59
nsubj:outer    0.00    0.00    0.00
nsubj:pass    78.12   78.95   78.53
advcl:cmp     58.82   47.62   52.63
fixed         53.33   72.73   61.54
csubj:pass     0.00    0.00    0.00
obl:agent     86.67   59.09   70.27
aux            0.00    0.00    0.00
aux:pass       0.00    0.00    0.00
flat:name     85.71   85.71   85.71

Evaluation finished
